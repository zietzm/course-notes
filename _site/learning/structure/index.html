<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Structure Learning for Bayesian Networks</title>
  <meta name="description" content="Lecture notes for Stanford cs228.">

  <link rel="stylesheet" href="/cs228-notes/css/tufte.css">

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not--><link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'><!-- Load up MathJax script if needed ... specify in /_data/options.yml file--><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_CHTML"></script><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75587219-1', 'auto');
  ga('send', 'pageview');
  </script>

  <link rel="canonical" href="/cs228-notes/learning/structure/">
  <link rel="alternate" type="application/rss+xml" title="Probabilistic graphical modeling course" href="/cs228-notes/feed.xml" />
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
    <nav class="group">
    <a href="/cs228-notes/">Contents</a>
    <a href="http://cs228.stanford.edu/">Class</a>
    <a href="http://github.com/ermongroup/cs228-notes">GitHub</a>
    </nav>
</header>

    <article class="group">
      <h1>Structure learning for bayesian networks</h1>
<p class="subtitle"></p>


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      Macros: {
        e: "\\epsilon",
        xti: "x^{(i)}",
        yti: "y^{(i)}",
        bfy: "{\\bf y}",
        bfx: "{\\bf x}",
        bfg: "{\\bf g}",
        bfbeta: "{\\bf \\beta}",
        tp: "\\tilde p",
        pt: "p_\\theta",
        E: "{\\mathbb{E}}",
        Ind: "{\\mathbb{I}}",
        KL: "{\\mathbb{KL}}",
        Re: "{\\mathbb{R}}",
        Dc: "{\\mathcal{D}}",
        Tc: "{\\mathcal{T}}",
        Xc: "{\\mathcal{X}}",
        note: ["\\textcolor{blue}{[NOTE: #1]}",1]
      }
    }
  });
</script>


<h2 id="structure-learning-for-bayesian-networks">Structure learning for Bayesian networks</h2>

<p>The task of structure learning for Bayesian networks refers to learn the structure of the directed acyclic graph (DAG) from data. There are two major approaches for the structure learning: score-based approach and constraint-based approach .</p>

<h3 id="score-based-approach">Score-based approach</h3>

<p>The score-based approach first defines a criterion to evaluate how well the Bayesian network fits the data, then searches over the space of DAGs for a structure with maximal score. In this way, the score-based approach is essentially a search problem and consists of two parts: the definition of score metric and the search algorithm.</p>

<h3 id="score-metrics">Score metrics</h3>

<p>The score metrics for a structure <script type="math/tex">\mathcal{G}</script> and data <script type="math/tex">D</script> can be generally defined as:</p>

<script type="math/tex; mode=display">Score(G:D)= LL(G:D) - \phi(|D|) \|G\|.</script>

<p>Here <script type="math/tex">LL(G:D)</script> refers to the log-likelihood of the data under the graph structure <script type="math/tex">\mathcal{G}.</script> The parameters in Bayesian network <script type="math/tex">G</script> are estimated based on MLE and the log-likelihood score is calculated based on the estimated parameters. If we consider only the log-likelihood in the score function, we will end up with an overfitting structure (namely, a complete graph.) That is why we have the second term in the scoring function. <script type="math/tex">\lvert D \rvert</script> is the number of sample and <script type="math/tex">\|G\|</script> is the number of parameters in the graph <script type="math/tex">\mathcal{G}</script>. With this extra term, we will penalize the over-complicated graph structure and avoid overfitting. For AIC the function <script type="math/tex">\phi(t) = 1,</script> while for BIC <script type="math/tex">\phi(t) = \log(t)/2.</script> It is important to note that in BIC, the influence of model complexity will decrease as M grows, allowing the log-likelihood term to eventually dominate the score.</p>

<p>There is another family of Bayesian score function called BD (Bayesian Dirichlet) score. For BD score, if first defines the probability of data <script type="math/tex">D</script> conditional on the graph structure <script type="math/tex">\mathcal{G}</script> as</p>

<script type="math/tex; mode=display">P(D|\mathcal{G})=\int P(D|\mathcal{G}, \Theta_{\mathcal{G}})P(\Theta_{\mathcal{G}}|\mathcal{G})d\Theta_{\mathcal{G}},</script>

<p>where <script type="math/tex">P(D \mid \mathcal{G}, \Theta_{\mathcal{G}})</script> is the probability of the data given the network structure and parameters, and <script type="math/tex">P(\Theta_{\mathcal{G}} \mid \mathcal{G})</script> is the prior probability of the parameters. When the prior probability is specified as a Dirichlet distribution,</p>

<script type="math/tex; mode=display">P(D|\Theta_{\mathcal{G}}) = \prod_{i} \prod_{\pi_i} \left[ \frac{\Gamma(\sum_j N'_{i,\pi_i,j})}{\Gamma(\sum_j N'_{i,\pi_i,j} + N_{i,\pi_i,j} )} \prod_{j}\frac{\Gamma(N'_{i,\pi_i,j} + N_{i,\pi_i,j})}{\Gamma(N'_{i,\pi_i,j})}\right].</script>

<p>Here <script type="math/tex">\pi_i</script> refers to the parent configuration of the variable <script type="math/tex">i</script> and <script type="math/tex">N_{i,\pi_i,j}</script> is the count of variable <script type="math/tex">i</script> taking value <script type="math/tex">j</script> with parent configuration <script type="math/tex">\pi_i</script>. <script type="math/tex">N'</script> represents the counts in the prior respectively.</p>

<p>With a prior for the graph structure <script type="math/tex">P(\Theta_{\mathcal{G}})</script> (say, a uniform one), the BD score is defined as</p>

<script type="math/tex; mode=display">\log P(D|\Theta_{\mathcal{G}}) + \log P(\Theta_{\mathcal{G}}).</script>

<p>Notice there is no penalty term appending to the BD score due to that it will penalize the overfitting implicitly via the integral over parameter space.</p>

<h3 id="chow-liu-algorithm">Chow-Liu Algorithm</h3>

<p>The Chow-Liu Algorithm is a specific type of score based approach. The Chow-Liu algorithm finds the maximum-likelihood tree structure where each node has at most one parent. Note that here our score is simply the maximum likelihood, we do not need to penalize the complexity since we are already limiting complexity by restricting ourselves to tree structures.</p>

<p>The algorithm has three steps:</p>

<p>1) Compute the mutual information for all pairs of variables <script type="math/tex">X,U</script>, and form the mutual information graph where the edge between variables <script type="math/tex">X,U</script> has weight <script type="math/tex">MI(X,U)</script>:</p>

<script type="math/tex; mode=display">MI(X,U) =\sum_{x,u} \hat p(x,u)\log\left[\frac{\hat{p} (x,u)}{\hat p(x) \hat p(u)}\right]</script>

<figure class="maincolumn">
    <figcaption class="marginnote"></figcaption><img src="/cs228-notes/assets/img/mi-graph.png" alt="" /></figure>

<p>Remember that from our empirical distribution <script type="math/tex">\hat p(x,u) = \frac{Count(x,u)}{\# \text{ data points}}</script>.</p>

<p>2) Find the <strong>maximum</strong> weight spanning tree: the maximal-weight tree that connects all vertices in a graph. This can be found using Kruskal or Prim Algorithms.</p>

<figure class="maincolumn">
    <figcaption class="marginnote"></figcaption><img src="/cs228-notes/assets/img/max-spanning-tree.png" alt="" /></figure>

<p>3) Pick any node to be the <em>root variable</em>, and assign directions radiating outward from this node (arrows go away from it). This step transforms the resulting undirected tree to a directed one.</p>

<figure class="maincolumn">
    <figcaption class="marginnote"></figcaption><img src="/cs228-notes/assets/img/chow-liu-tree.png" alt="" /></figure>

<p>The Chow-Liu Algorithm has a complexity of order <script type="math/tex">n^2</script>, as it takes <script type="math/tex">O(n^2)</script> to compute mutual information for all pairs, and <script type="math/tex">O(n^2)</script> to compute the maximum spanning tree.</p>

<p>Now that we have described the algorithm, lets explain why this works. It turns out that the likelihood score decomposes into mutual information and entropy terms:</p>

<script type="math/tex; mode=display">\log p(\mathcal D\mid \theta^{ML},G) = |\mathcal D| \sum_i MI_{\hat p}(X_i,X_{pa(i)}) - |\mathcal D| \sum_i H_{\hat p}(X_i)</script>

<p>We would like to find a graph <script type="math/tex">G</script> that maximizes this log-likelihood. Since the entropies are independent of the dependency ordering in the tree, the only terms that change with choice of <script type="math/tex">G</script> are the mutual information terms. So we want</p>

<script type="math/tex; mode=display">\arg\max_G \log P(\mathcal D\mid \theta^{ML}(G),G) = \arg\max_G\sum_i MI(X_i,X_{pa(i)})</script>

<p>Now if we assume <script type="math/tex">G = (V,E)</script> is a tree where each node has at most one parent, we get</p>

<script type="math/tex; mode=display">\arg\max_{G:G\text{ is tree}} \log P(\mathcal D\mid \theta^{ML}(G),G) = \arg\max_{G:G\text{ is tree}}\sum_{(i,j)\in E} MI(X_i,X_j)</script>

<p>Note that the orientations of edges do not matter because mutual information is symmetric. Thus we can see why the Chow-Liu algorithm finds the best approximate tree structure where nodes are restricted to have at most one parent.</p>

<h3 id="search-algorithms">Search algorithms</h3>

<p>The most common choice for search algorithms are local search and greedy search.</p>

<p>For local search algorithm, it starts with an empty graph or a complete graph. At each step, it attempts to change the graph structure by a single operation of adding an edge, removing an edge or reversing an edge. (Of course, the operation should preserve the acyclic property.) If the score increases, then it adopts the attempt and does the change, otherwise it makes another attempt.</p>

<p>For greedy search (namely the K3 algorithm), we first assume a topological order of the graph. For each variable, we restrict its parent set to the variables with a higher order. While searching for parent set for each variable, it takes a greedy approach by adding the parent that increases the score most until no improvement can be made.</p>

<p>A former CS228 student has created an <a href="http://pgmlearning.herokuapp.com/k3LearningApp">interactive web simulation</a> for visualizing the K3 learning algorithm. Feel free to play around with it and, if you do, please submit any feedback or bugs through the Feedback button on the web app.</p>

<p>Although both approach are computational tractable, neither of them have a guarantee of the quality of the graph that we end up with. The graph space is highly “non-convex” and both algorithm might get stuck at some sub-optimal regions.</p>

<h3 id="constraint-based-approach">Constraint-based approach</h3>

<p>The constraint-based case employs the independence test to identify a set of edge constraints for the graph and then finds the best DAG that satisfies the constraints. For example, we could distinguish V-structure and fork-structure by doing an independence test for the two variables on the sides conditional on the variable in the middle. This approach works well with some other prior (expert) knowledge of structure but requires lots of data samples to guarantee testing power. So it is less reliable when the number of sample is small.</p>

<h3 id="recent-advances">Recent Advances</h3>

<p>In this section, we will briefly introduce two recent algorithms for graph search: order-search (OS) approach and integer linear programming (ILP) approach.</p>

<p>The OS approach, as its name suggests, conducts a search over the topological orders and the graph space at the same time. The K3 algorithm assumes a topological order in advance and searches only over the graphs that obey the topological order. When the order specified is a poor one, it may end with a bad graph structure (with a low graph score). The OS algorithm resolves this problem by performing a search over orders at the same time. It swaps the order of two adjacent variables at each step and employs the K3 algorithm as a sub-routine.</p>

<p>The ILP approach encodes the graph structure, scoring and the acyclic constraints into a linear programming problem. Thus it can utilize a state-of-art integer programming solver. That said, this approach requires a bound on the maximum number of parents any node in the graph can have (say to be 4 or 5). Otherwise, the number of constraints in the ILP will explode and the computation will become intractable.</p>

<p><br /></p>

<table>
  <tbody>
    <tr>
      <td><a href="../../">Index</a></td>
      <td><a href="../bayesian">Previous</a></td>
      <td><a href="../../extras/vae">Next</a></td>
    </tr>
  </tbody>
</table>


    </article>
    <footer>
<hr class="slender">
<div class="credits">
Site created with <a href="//jekyllrb.com">Jekyll</a> using the <a href="//github.com/clayh53/tufte-jekyll">Tufte theme</a>. &copy; 2019
</div>
</footer>

  </body>
</html>
