I"Ž<p><span class="newthought">These notes</span> form a concise introductory course on probabilistic graphical models<label for="note-pgm" class="margin-toggle sidenote-number"></label><input type="checkbox" id="note-pgm" class="margin-toggle" /><span class="sidenote">Probabilistic graphical models are a subfield of machine learning that studies how to describe and reason about the world in terms of probabilities.</span>.
They are based on Stanford <a href="https://cs228.stanford.edu/">CS228</a>, and are written by <a href="http://www.stanford.edu/~kuleshov">Volodymyr Kuleshov</a> and <a href="http://cs.stanford.edu/~ermon/">Stefano Ermon</a>, with the <a href="https://github.com/ermongroup/cs228-notes/commits/master">help</a> of many students and course staff.
<label for="mn-construction" class="margin-toggle">âŠ•</label><input type="checkbox" id="mn-construction" class="margin-toggle" /><span class="marginnote">The notes are still <strong>under construction</strong>! Although we have written up most of the material, you will probably find several typos. If you do, please let us know, or submit a pull request with your fixes to our <a href="https://github.com/ermongroup/cs228-notes">GitHub repository</a>.</span>
You too may help make these notes better by submitting your improvements to us via <a href="https://github.com/ermongroup/cs228-notes">GitHub</a>.</p>

<p>This course starts by introducing probabilistic graphical models from the very basics and concludes by explaining from first principles the <a href="extras/vae">variational auto-encoder</a>, an important probabilistic model that is also one of the most influential recent results in deep learning.</p>

<h2 id="preliminaries">Preliminaries</h2>

<ol>
  <li>
    <p><a href="preliminaries/introduction/">Introduction</a>: What is probabilistic graphical modeling? Overview of the course.</p>
  </li>
  <li>
    <p><a href="preliminaries/probabilityreview">Review of probability theory</a>: Probability distributions. Conditional probability. Random variables (<em>under construction</em>).</p>
  </li>
  <li>
    <p><a href="preliminaries/applications">Examples of real-world applications</a>: Image denoising. RNA structure prediction. Syntactic analysis of sentences. Optical character recognition (<em>under construction</em>).</p>
  </li>
</ol>

<h2 id="representation">Representation</h2>

<ol>
  <li>
    <p><a href="representation/directed/">Bayesian networks</a>: Definitions. Representations via directed graphs. Independencies in directed models.</p>
  </li>
  <li>
    <p><a href="representation/undirected/">Markov random fields</a>: Undirected vs directed models. Independencies in undirected models. Conditional random fields.</p>
  </li>
</ol>

<h2 id="inference">Inference</h2>

<ol>
  <li>
    <p><a href="inference/ve/">Variable elimination</a> The inference problem. Variable elimination. Complexity of inference.</p>
  </li>
  <li>
    <p><a href="inference/jt/">Belief propagation</a>: The junction tree algorithm. Exact inference in arbitrary graphs. Loopy Belief Propagation.</p>
  </li>
  <li>
    <p><a href="inference/map/">MAP inference</a>: Max-sum message passing. Graphcuts. Linear programming relaxations. Dual decomposition.</p>
  </li>
  <li>
    <p><a href="inference/sampling/">Sampling-based inference</a>: Monte-Carlo sampling. Forward Sampling. Rejection Sampling. Importance sampling. Markov Chain Monte-Carlo. Applications in inference.</p>
  </li>
  <li>
    <p><a href="inference/variational/">Variational inference</a>: Variational lower bounds. Mean Field. Marginal polytope and its relaxations.</p>
  </li>
</ol>

<h2 id="learning">Learning</h2>

<ol>
  <li>
    <p><a href="learning/directed/">Learning in directed models</a>: Maximum likelihood estimation. Learning theory basics. Maximum likelihood estimators for Bayesian networks.</p>
  </li>
  <li>
    <p><a href="learning/undirected/">Learning in undirected models</a>: Exponential families. Maximum likelihood estimation with gradient descent. Learning in CRFs</p>
  </li>
  <li>
    <p><a href="learning/latent/">Learning in latent variable models</a>: Latent variable models. Gaussian mixture models. Expectation maximization.</p>
  </li>
  <li>
    <p><a href="learning/bayesian/">Bayesian learning</a>: Bayesian paradigm. Conjugate priors. Examples (<em>under construction</em>).</p>
  </li>
  <li>
    <p><a href="learning/structure/">Structure learning</a>: Chow-Liu algorithm. Akaike information criterion. Bayesian information criterion. Bayesian structure learning (<em>under construction</em>).</p>
  </li>
</ol>

<h2 id="bringing-it-all-together">Bringing it all together</h2>

<ol>
  <li>
    <p><a href="extras/vae">The variational autoencoder</a>: Deep generative models. The reparametrization trick. Learning latent visual representations.</p>
  </li>
  <li>
    <p><a href="extras/readings">List of further readings</a>: Structured support vector machines. Bayesian non-parametrics.</p>
  </li>
</ol>
:ET